{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dcn-demo.ipynb의 사본","provenance":[{"file_id":"1IAhnHihvooCRphA48oEf1tx-YjU2wmkj","timestamp":1617891092212}],"collapsed_sections":[],"authorship_tag":"ABX9TyNzTOxckhM7mO1bEB9uiTs7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"z5QfYnKji-Iu"},"source":["import torch\n","import numpy as np\n","import random\n","def setup_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed) # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    # torch.backends.cudnn.benchmark = True\n","    np.random.seed(seed)\n","    random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdUVVBpZHh2a"},"source":["import torch\n","import torchvision.ops\n","from torch import nn\n","\n","class DeformableConv2d(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 kernel_size=3,\n","                 stride=1,\n","                 padding=1,\n","                 bias=False):\n","\n","        super(DeformableConv2d, self).__init__()\n","\n","        self.padding = padding\n","        \n","        self.offset_conv = nn.Conv2d(in_channels, \n","                                     2 * kernel_size * kernel_size,\n","                                     kernel_size=kernel_size, \n","                                     stride=stride,\n","                                     padding=self.padding, \n","                                     bias=True)\n","\n","        nn.init.constant_(self.offset_conv.weight, 0.)\n","        nn.init.constant_(self.offset_conv.bias, 0.)\n","        \n","        self.modulator_conv = nn.Conv2d(in_channels, \n","                                     1 * kernel_size * kernel_size,\n","                                     kernel_size=kernel_size, \n","                                     stride=stride,\n","                                     padding=self.padding, \n","                                     bias=True)\n","\n","        nn.init.constant_(self.modulator_conv.weight, 0.)\n","        nn.init.constant_(self.modulator_conv.bias, 0.)\n","        \n","        self.regular_conv = nn.Conv2d(in_channels=in_channels,\n","                                      out_channels=out_channels,\n","                                      kernel_size=kernel_size,\n","                                      stride=stride,\n","                                      padding=self.padding,\n","                                      bias=bias)\n","    \n","    def forward(self, x):\n","        h, w = x.shape[2:]\n","        max_offset = max(h, w)/4.\n","\n","        offset = self.offset_conv(x).clamp(-max_offset, max_offset)\n","        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n","        \n","        x = torchvision.ops.deform_conv2d(input=x, \n","                                          offset=offset, \n","                                          weight=self.regular_conv.weight, \n","                                          bias=self.regular_conv.bias, \n","                                          padding=self.padding,\n","                                          mask=modulator\n","                                          )\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFXGwYFAHsGs"},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","\n","\n","import torch\n","from torch import nn\n","\n","\n","class MNISTClassifier(nn.Module):\n","    def __init__(self,\n","                 deformable=False):\n","\n","        super(MNISTClassifier, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=True)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)   \n","        conv = nn.Conv2d if deformable==False else DeformableConv2d\n","        self.conv4 = conv(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n","        self.conv5 = conv(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n","        \n","        self.pool = nn.MaxPool2d(2)\n","        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(32, 10)\n","        \n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = self.pool(x) # [14, 14]\n","        x = torch.relu(self.conv2(x))\n","        x = self.pool(x) # [7, 7]\n","        x = torch.relu(self.conv3(x))\n","        x = torch.relu(self.conv4(x))\n","        x = torch.relu(self.conv5(x))\n","        x = self.gap(x)\n","        x = x.flatten(start_dim=1)\n","        x = self.fc(x)\n","        return x\n","\n","\n","def train(model, loss_function, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_function(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","def test(model, loss_function, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    num_data = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            org_data, target = data.to(device), target.to(device)\n","\n","            for scale in np.arange(0.5, 1.6, 0.1): # [0.5, 0.6, ... ,1.2, 1.3, 1.4, 1.5]\n","                data = transforms.functional.affine(org_data, scale=scale, angle=0, translate=[0,0],shear=0)\n","                output = model(data)\n","                test_loss += loss_function(output, target).item()  # sum up batch mean loss\n","                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","                num_data += len(data)\n","\n","    test_loss /= num_data\n","\n","    test_acc = 100. * correct / num_data\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, num_data,\n","        test_acc))\n","    return test_acc\n","\n","\n","def main(use_deformable_conv=False):\n","    # Training settings\n","    seed=1\n","    setup_seed(seed)\n","\n","    use_cuda = torch.cuda.is_available()\n","    batch_size = 64\n","    lr=1e-3\n","    gamma=0.7\n","    epochs=14\n","\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    train_kwargs = {'batch_size': batch_size}\n","    test_kwargs = {'batch_size': batch_size}\n","    if use_cuda:\n","        cuda_kwargs = {'num_workers': 4,\n","                       'pin_memory': True,\n","                       'shuffle': True}\n","        train_kwargs.update(cuda_kwargs)\n","        test_kwargs.update(cuda_kwargs)\n","\n","    train_transform = transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    \n","    transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","    \n","    dataset1 = datasets.MNIST('./data', train=True, download=True,\n","                       transform=train_transform)\n","    dataset2 = datasets.MNIST('./data', train=False,\n","                       transform=transform)\n","    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n","    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n","\n","    model = MNISTClassifier(use_deformable_conv).to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n","    loss_function = nn.CrossEntropyLoss()\n","    best_test_acc = 0.\n","    for epoch in range(1, epochs + 1):\n","        train(model, loss_function, device, train_loader, optimizer, epoch)\n","        best_test_acc = max(best_test_acc, test(model, loss_function, device, test_loader))\n","        scheduler.step()\n","    print(\"best top1 acc(%): \", f\"{best_test_acc:.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDh26cxyKax5","executionInfo":{"status":"ok","timestamp":1617890129011,"user_tz":-540,"elapsed":189474,"user":{"displayName":"권용혜","photoUrl":"","userId":"13973451055019228775"}},"outputId":"6082fbac-7806-465b-8d2f-278371a7bfd2"},"source":["main(use_deformable_conv=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0101, Accuracy: 88833/110000 (80.76%)\n","\n","\n","Test set: Average loss: 0.0138, Accuracy: 83529/110000 (75.94%)\n","\n","\n","Test set: Average loss: 0.0078, Accuracy: 93126/110000 (84.66%)\n","\n","\n","Test set: Average loss: 0.0065, Accuracy: 96733/110000 (87.94%)\n","\n","\n","Test set: Average loss: 0.0066, Accuracy: 96024/110000 (87.29%)\n","\n","\n","Test set: Average loss: 0.0060, Accuracy: 97076/110000 (88.25%)\n","\n","\n","Test set: Average loss: 0.0055, Accuracy: 98812/110000 (89.83%)\n","\n","\n","Test set: Average loss: 0.0056, Accuracy: 98281/110000 (89.35%)\n","\n","\n","Test set: Average loss: 0.0056, Accuracy: 98391/110000 (89.45%)\n","\n","\n","Test set: Average loss: 0.0058, Accuracy: 97774/110000 (88.89%)\n","\n","\n","Test set: Average loss: 0.0053, Accuracy: 99030/110000 (90.03%)\n","\n","\n","Test set: Average loss: 0.0056, Accuracy: 98597/110000 (89.63%)\n","\n","\n","Test set: Average loss: 0.0055, Accuracy: 98845/110000 (89.86%)\n","\n","\n","Test set: Average loss: 0.0055, Accuracy: 98733/110000 (89.76%)\n","\n","best top1 acc(%):  90.03\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gdDR8d9OKcOp","executionInfo":{"status":"ok","timestamp":1617890361052,"user_tz":-540,"elapsed":421503,"user":{"displayName":"권용혜","photoUrl":"","userId":"13973451055019228775"}},"outputId":"3a8f4539-3f5f-49fc-973c-8002c3fa76c8"},"source":["main(use_deformable_conv=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Test set: Average loss: 0.0067, Accuracy: 94507/110000 (85.92%)\n","\n","\n","Test set: Average loss: 0.0048, Accuracy: 99084/110000 (90.08%)\n","\n","\n","Test set: Average loss: 0.0047, Accuracy: 99246/110000 (90.22%)\n","\n","\n","Test set: Average loss: 0.0040, Accuracy: 100938/110000 (91.76%)\n","\n","\n","Test set: Average loss: 0.0044, Accuracy: 100445/110000 (91.31%)\n","\n","\n","Test set: Average loss: 0.0043, Accuracy: 100963/110000 (91.78%)\n","\n","\n","Test set: Average loss: 0.0041, Accuracy: 100745/110000 (91.59%)\n","\n","\n","Test set: Average loss: 0.0039, Accuracy: 101710/110000 (92.46%)\n","\n","\n","Test set: Average loss: 0.0036, Accuracy: 102096/110000 (92.81%)\n","\n","\n","Test set: Average loss: 0.0036, Accuracy: 102070/110000 (92.79%)\n","\n","\n","Test set: Average loss: 0.0038, Accuracy: 101747/110000 (92.50%)\n","\n","\n","Test set: Average loss: 0.0035, Accuracy: 102195/110000 (92.90%)\n","\n","\n","Test set: Average loss: 0.0037, Accuracy: 101947/110000 (92.68%)\n","\n","\n","Test set: Average loss: 0.0037, Accuracy: 102001/110000 (92.73%)\n","\n","best top1 acc(%):  92.90\n"],"name":"stdout"}]}]}